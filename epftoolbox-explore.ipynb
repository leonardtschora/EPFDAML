{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Electricity price forecasting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import epftoolbox as epf\n",
    "from epftoolbox.data import read_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test datasets: 2016-01-01 00:00:00 - 2016-02-01 23:00:00\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "df_train, df_test = read_data(\n",
    "    path='data_epf', dataset='FR', begin_test_date='01-01-2016',\n",
    "    end_test_date='01-02-2016')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Exogenous 1</th>\n",
       "      <th>Exogenous 2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-01 00:00:00</th>\n",
       "      <td>23.86</td>\n",
       "      <td>58088.0</td>\n",
       "      <td>59108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 01:00:00</th>\n",
       "      <td>22.39</td>\n",
       "      <td>56761.0</td>\n",
       "      <td>54623.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 02:00:00</th>\n",
       "      <td>20.59</td>\n",
       "      <td>55911.0</td>\n",
       "      <td>54673.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 03:00:00</th>\n",
       "      <td>16.81</td>\n",
       "      <td>51949.0</td>\n",
       "      <td>50913.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 04:00:00</th>\n",
       "      <td>17.41</td>\n",
       "      <td>51773.0</td>\n",
       "      <td>49368.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Price  Exogenous 1  Exogenous 2\n",
       "Date                                                \n",
       "2016-01-01 00:00:00  23.86      58088.0      59108.0\n",
       "2016-01-01 01:00:00  22.39      56761.0      54623.0\n",
       "2016-01-01 02:00:00  20.59      55911.0      54673.0\n",
       "2016-01-01 03:00:00  16.81      51949.0      50913.0\n",
       "2016-01-01 04:00:00  17.41      51773.0      49368.0"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 26280 entries, 2013-01-01 00:00:00 to 2015-12-31 23:00:00\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   Price        26280 non-null  float64\n",
      " 1   Exogenous 1  26280 non-null  float64\n",
      " 2   Exogenous 2  26280 non-null  float64\n",
      "dtypes: float64(3)\n",
      "memory usage: 821.2 KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2013-01-01 00:00:00', '2013-01-01 01:00:00',\n",
       "               '2013-01-01 02:00:00', '2013-01-01 03:00:00',\n",
       "               '2013-01-01 04:00:00', '2013-01-01 05:00:00',\n",
       "               '2013-01-01 06:00:00', '2013-01-01 07:00:00',\n",
       "               '2013-01-01 08:00:00', '2013-01-01 09:00:00',\n",
       "               ...\n",
       "               '2015-12-31 14:00:00', '2015-12-31 15:00:00',\n",
       "               '2015-12-31 16:00:00', '2015-12-31 17:00:00',\n",
       "               '2015-12-31 18:00:00', '2015-12-31 19:00:00',\n",
       "               '2015-12-31 20:00:00', '2015-12-31 21:00:00',\n",
       "               '2015-12-31 22:00:00', '2015-12-31 23:00:00'],\n",
       "              dtype='datetime64[ns]', name='Date', length=26280, freq=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, cols_countries, pivot=False):\n",
    "\n",
    "    # get the FR and DE string from cols_FR_DE\n",
    "    countries = [col.split('_')[0] for col in cols_countries]\n",
    "    countries = list(set(countries))\n",
    "    countries.remove('DAH') \n",
    "    # Convert the date to datetime   \n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    # Extracting Year, Month, Day, Day of Week, and Hour\n",
    "    df['Year'] = df['date'].dt.year\n",
    "    df['Month'] = df['date'].dt.month\n",
    "    df['Day'] = df['date'].dt.day\n",
    "    df['DayOfWeek'] = df['date'].dt.day_name()\n",
    "    df['Hour'] = df['date'].dt.hour\n",
    "    df['Hour'] = df['Hour'].apply(lambda x: str(x + 1).zfill(2) if x < 23 else '00')\n",
    "    df.insert(0, 'Hour', df.pop('Hour'))\n",
    "    df.insert(0, 'DayOfWeek', df.pop('DayOfWeek'))\n",
    "    df.insert(0, 'Day', df.pop('Day'))\n",
    "    df.insert(0, 'Month', df.pop('Month'))\n",
    "    df.insert(0, 'Year', df.pop('Year'))\n",
    "\n",
    "    # Dataset for FR and DE \n",
    "    \n",
    "    df_FR_DE = df[['Year', 'Month', 'Day', 'DayOfWeek', 'Hour', 'date'] + cols_countries]\n",
    "\n",
    "    # We drop the NaN values \n",
    "    df_FR_DE.dropna(how='all', inplace=True)\n",
    "    \n",
    "    if pivot is True:\n",
    "\n",
    "        # Colonne Y_M_D pour pouvoir grouper les donnÃ©es sur un mÃªme jour\n",
    "        df_FR_DE['Y_M_D'] = df_FR_DE['Year'].astype('str') + \"_\" + df_FR_DE['Month'].astype('str') + \"_\" + df_FR_DE['Day'].astype('str')\n",
    "        df_FR_DE['Y_M_D_H'] = df_FR_DE['Year'].astype('str') + \"_\" + df_FR_DE['Month'].astype('str') + \"_\" + df_FR_DE['Day'].astype('str')+ \"_\" + df_FR_DE['Hour'].astype('str')\n",
    "        # On supprime les duplicates\n",
    "        df_FR_DE.drop_duplicates(subset=[\"Y_M_D_H\"], inplace=True)\n",
    "\n",
    "        # Ajout des colonnes FR_Spot_J1 et DE_Spot_J1\n",
    "        df_FR_DE_J1 = df_FR_DE.copy()[[countries[0]+\"_Spot\", countries[1]+\"_Spot\", \"date\"]]\n",
    "\n",
    "        df_FR_DE_J1[\"date+1\"] = df_FR_DE_J1[\"date\"] + datetime.timedelta(days=1)\n",
    "        df_FR_DE_J1.rename(columns={countries[0]+\"_Spot\":countries[0]+\"_Spot_J1\", countries[1]+\"_Spot\":countries[1]+\"_Spot_J1\"}, inplace=True)\n",
    "        df_FR_DE = pd.merge(df_FR_DE, df_FR_DE_J1.drop(columns=['date']), left_on=\"date\", right_on=\"date+1\")\n",
    "\n",
    "        # Table pivot : 1 ligne = 1 jour avec 24*10 features pour chaque heure de la journÃ©e\n",
    "        df_FR_DE_pivot = pd.pivot_table(df_FR_DE, columns=[\"Hour\"], values=cols_countries+[countries[0]+'_Spot_J1', countries[1]+'_Spot_J1'], index=['Y_M_D'], aggfunc='sum')\n",
    "        df_FR_DE_pivot.columns = df_FR_DE_pivot.columns.to_flat_index()\n",
    "        df_FR_DE_pivot.reset_index(inplace=True)\n",
    "        list_col = df_FR_DE_pivot.columns.values.tolist()\n",
    "        new_cols = []\n",
    "        for col in list_col:\n",
    "            col = str(col)\n",
    "            col = col.replace('(', '').replace(')', '').replace(\",\", \"\").replace(\" \", \"_\").replace(\"'\", \"\")\n",
    "            new_cols.append(col)\n",
    "            \n",
    "        df_FR_DE_pivot.columns = new_cols\n",
    "\n",
    "        df_FR_DE_pivot.dropna(inplace=True)\n",
    "        df_FR_DE = df_FR_DE_pivot\n",
    "        \n",
    "\n",
    "    return df_FR_DE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we load data from Olivier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matis\\AppData\\Local\\Temp\\ipykernel_26620\\60855226.py:1: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv('data\\Data_Europe_DAH_2016_2023.csv', index_col=0, parse_dates=True)\n",
      "C:\\Users\\matis\\AppData\\Local\\Temp\\ipykernel_26620\\1255989459.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_FR_DE.dropna(how='all', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data\\Data_Europe_DAH_2016_2023.csv', index_col=0, parse_dates=True)\n",
    "# Choose the countries and the columns\n",
    "cols_FR_DE = [\n",
    "    'FR_Spot', 'FR_Load', 'DE_Spot', 'DE_Load',\n",
    "    'FR_DAH_generation', 'DE_DAH_generation',\n",
    "    # 'FR_wind onshore_DAH', 'DE_wind onshore_DAH', \n",
    "    'DAH_ImportCapacity_FR_DE', 'DAH_ImportCapacity_DE_FR'\n",
    "             ]\n",
    "# Pivot = True pour avoir une ligne par jour\n",
    "df_FR_DE = preprocess(df, cols_FR_DE, pivot=False)\n",
    "df_FR_DE.set_index('date', inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-01-01 00:00:00\n",
      "2023-12-31 23:00:00\n"
     ]
    }
   ],
   "source": [
    "min_date = df_FR_DE.index.min()\n",
    "print(min_date)\n",
    "max_date = df_FR_DE.index.max()\n",
    "print(max_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical columns\n",
    "cols = df_FR_DE.columns\n",
    "num_cols = list(df_FR_DE._get_numeric_data().columns)\n",
    "num_cols.remove('Year')\n",
    "num_cols.remove('Month')\n",
    "num_cols.remove('Day')\n",
    "cat_cols = list(set(cols) - set(num_cols))\n",
    "df_FR_DE_num = df_FR_DE[num_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FR_Spot</th>\n",
       "      <th>FR_Load</th>\n",
       "      <th>DE_Spot</th>\n",
       "      <th>DE_Load</th>\n",
       "      <th>FR_DAH_generation</th>\n",
       "      <th>DE_DAH_generation</th>\n",
       "      <th>DAH_ImportCapacity_FR_DE</th>\n",
       "      <th>DAH_ImportCapacity_DE_FR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-12-31 19:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-31 20:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-31 21:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-31 22:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-31 23:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     FR_Spot  FR_Load  DE_Spot  DE_Load  FR_DAH_generation  \\\n",
       "date                                                                         \n",
       "2023-12-31 19:00:00      NaN      NaN      NaN      NaN                NaN   \n",
       "2023-12-31 20:00:00      NaN      NaN      NaN      NaN                NaN   \n",
       "2023-12-31 21:00:00      NaN      NaN      NaN      NaN                NaN   \n",
       "2023-12-31 22:00:00      NaN      NaN      NaN      NaN                NaN   \n",
       "2023-12-31 23:00:00      NaN      NaN      NaN      NaN                NaN   \n",
       "\n",
       "                     DE_DAH_generation  DAH_ImportCapacity_FR_DE  \\\n",
       "date                                                               \n",
       "2023-12-31 19:00:00                NaN                       NaN   \n",
       "2023-12-31 20:00:00                NaN                       NaN   \n",
       "2023-12-31 21:00:00                NaN                       NaN   \n",
       "2023-12-31 22:00:00                NaN                       NaN   \n",
       "2023-12-31 23:00:00                NaN                       NaN   \n",
       "\n",
       "                     DAH_ImportCapacity_DE_FR  \n",
       "date                                           \n",
       "2023-12-31 19:00:00                       NaN  \n",
       "2023-12-31 20:00:00                       NaN  \n",
       "2023-12-31 21:00:00                       NaN  \n",
       "2023-12-31 22:00:00                       NaN  \n",
       "2023-12-31 23:00:00                       NaN  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_FR_DE_num.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format dataframe with columns (Price, Exogenous 1, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns except DE_Spot\n",
    "cols_FR = [col for col in df_FR_DE_num.columns if col != 'DE_Spot']\n",
    "df_FR = df_FR_DE_num[cols_FR]\n",
    "# list of columns (Price, Exogenous1, Exogenous2, ...)\n",
    "new_cols= ['Price']+[f'Exogenous {i}' for i in range(1, len(cols_FR))]\n",
    "df_FR.columns = new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Exogenous 1</th>\n",
       "      <th>Exogenous 2</th>\n",
       "      <th>Exogenous 3</th>\n",
       "      <th>Exogenous 4</th>\n",
       "      <th>Exogenous 5</th>\n",
       "      <th>Exogenous 6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-01 00:00:00</th>\n",
       "      <td>22.39</td>\n",
       "      <td>56550.0</td>\n",
       "      <td>177664.36</td>\n",
       "      <td>62205.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 01:00:00</th>\n",
       "      <td>20.59</td>\n",
       "      <td>56150.0</td>\n",
       "      <td>171405.81</td>\n",
       "      <td>60615.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 02:00:00</th>\n",
       "      <td>16.81</td>\n",
       "      <td>52600.0</td>\n",
       "      <td>169234.01</td>\n",
       "      <td>56583.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 03:00:00</th>\n",
       "      <td>17.41</td>\n",
       "      <td>49750.0</td>\n",
       "      <td>169488.24</td>\n",
       "      <td>56541.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 04:00:00</th>\n",
       "      <td>17.02</td>\n",
       "      <td>48850.0</td>\n",
       "      <td>172496.31</td>\n",
       "      <td>56919.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Price  Exogenous 1  Exogenous 2  Exogenous 3  \\\n",
       "date                                                                \n",
       "2016-01-01 00:00:00  22.39      56550.0    177664.36      62205.0   \n",
       "2016-01-01 01:00:00  20.59      56150.0    171405.81      60615.0   \n",
       "2016-01-01 02:00:00  16.81      52600.0    169234.01      56583.5   \n",
       "2016-01-01 03:00:00  17.41      49750.0    169488.24      56541.0   \n",
       "2016-01-01 04:00:00  17.02      48850.0    172496.31      56919.0   \n",
       "\n",
       "                     Exogenous 4  Exogenous 5  Exogenous 6  \n",
       "date                                                        \n",
       "2016-01-01 00:00:00          NaN          NaN          NaN  \n",
       "2016-01-01 01:00:00          NaN          NaN          NaN  \n",
       "2016-01-01 02:00:00          NaN          NaN          NaN  \n",
       "2016-01-01 03:00:00          NaN          NaN          NaN  \n",
       "2016-01-01 04:00:00          NaN          NaN          NaN  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_FR.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill missing values with median per month "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matis\\AppData\\Local\\Temp\\ipykernel_26620\\2383202340.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_FR.fillna(df_FR_monthly.median(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Fill na with median of column on 1 month \n",
    "# Resample DataFrame to a monthly frequency\n",
    "df_FR_monthly = df_FR.resample('M').median()\n",
    "# Fill missing values with the median of each column calculated over a rolling window of 1 month\n",
    "df_FR.fillna(df_FR_monthly.median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Exogenous 1</th>\n",
       "      <th>Exogenous 2</th>\n",
       "      <th>Exogenous 3</th>\n",
       "      <th>Exogenous 4</th>\n",
       "      <th>Exogenous 5</th>\n",
       "      <th>Exogenous 6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-12-31 19:00:00</th>\n",
       "      <td>50.695</td>\n",
       "      <td>49537.5</td>\n",
       "      <td>232007.175</td>\n",
       "      <td>56211.125</td>\n",
       "      <td>53744.2975</td>\n",
       "      <td>971.525</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-31 20:00:00</th>\n",
       "      <td>50.695</td>\n",
       "      <td>49537.5</td>\n",
       "      <td>232007.175</td>\n",
       "      <td>56211.125</td>\n",
       "      <td>53744.2975</td>\n",
       "      <td>971.525</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-31 21:00:00</th>\n",
       "      <td>50.695</td>\n",
       "      <td>49537.5</td>\n",
       "      <td>232007.175</td>\n",
       "      <td>56211.125</td>\n",
       "      <td>53744.2975</td>\n",
       "      <td>971.525</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-31 22:00:00</th>\n",
       "      <td>50.695</td>\n",
       "      <td>49537.5</td>\n",
       "      <td>232007.175</td>\n",
       "      <td>56211.125</td>\n",
       "      <td>53744.2975</td>\n",
       "      <td>971.525</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-31 23:00:00</th>\n",
       "      <td>50.695</td>\n",
       "      <td>49537.5</td>\n",
       "      <td>232007.175</td>\n",
       "      <td>56211.125</td>\n",
       "      <td>53744.2975</td>\n",
       "      <td>971.525</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Price  Exogenous 1  Exogenous 2  Exogenous 3  \\\n",
       "date                                                                 \n",
       "2023-12-31 19:00:00  50.695      49537.5   232007.175    56211.125   \n",
       "2023-12-31 20:00:00  50.695      49537.5   232007.175    56211.125   \n",
       "2023-12-31 21:00:00  50.695      49537.5   232007.175    56211.125   \n",
       "2023-12-31 22:00:00  50.695      49537.5   232007.175    56211.125   \n",
       "2023-12-31 23:00:00  50.695      49537.5   232007.175    56211.125   \n",
       "\n",
       "                     Exogenous 4  Exogenous 5  Exogenous 6  \n",
       "date                                                        \n",
       "2023-12-31 19:00:00   53744.2975      971.525          0.0  \n",
       "2023-12-31 20:00:00   53744.2975      971.525          0.0  \n",
       "2023-12-31 21:00:00   53744.2975      971.525          0.0  \n",
       "2023-12-31 22:00:00   53744.2975      971.525          0.0  \n",
       "2023-12-31 23:00:00   53744.2975      971.525          0.0  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_FR.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FR.to_csv('datasets\\FR_readytodnn.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test split\n",
    "- 4 years to train\n",
    "- minimum 1 year for test\n",
    "- /!\\ 2021, 2022 prediction, covid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_date_test = '2023-09-27'\n",
    "end_date_test = '2023-11-27'\n",
    "df_test = df_FR_DE_num.loc[begin_date_test:end_date_test]\n",
    "df_train = df_FR_DE_num.loc[:begin_date_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling data with EPFtoolbox :\n",
    "\n",
    "    'Norm' for normalizing the data to the interval [0, 1].\n",
    "    'Norm1' for normalizing the data to the interval [-1, 1].\n",
    "    'Std' for standarizing the data to follow a normal distribution.\n",
    "    'Median' for normalizing the data based on the median as defined in as defined in here.\n",
    "    'Invariant' for scaling the data based on the asinh transformation (a variance stabilizing transformations) as defined in here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from epftoolbox.data import DataScaler\n",
    "Xtrain = df_train.values\n",
    "Xtest = df_test.values\n",
    "scaler = DataScaler('Norm')\n",
    "Xtrain_scaled = scaler.fit_transform(Xtrain)\n",
    "Xtest_scaled = scaler.transform(Xtest)\n",
    "\n",
    "# Xtrain_inverse = scaler.inverse_transform(Xtrain_scaled)\n",
    "# Xtest_inverse = scaler.inverse_transform(Xtest_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>FR_Spot</th>\n",
       "      <th>FR_Load</th>\n",
       "      <th>DE_Spot</th>\n",
       "      <th>DE_Load</th>\n",
       "      <th>FR_DAH_generation</th>\n",
       "      <th>DE_DAH_generation</th>\n",
       "      <th>DAH_ImportCapacity_FR_DE</th>\n",
       "      <th>DAH_ImportCapacity_DE_FR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-09-27 19:00:00</th>\n",
       "      <td>2023</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>106.28</td>\n",
       "      <td>45200.0</td>\n",
       "      <td>107.82</td>\n",
       "      <td>207487.66</td>\n",
       "      <td>48853.0</td>\n",
       "      <td>44660.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1812.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-27 20:00:00</th>\n",
       "      <td>2023</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>96.38</td>\n",
       "      <td>43500.0</td>\n",
       "      <td>96.38</td>\n",
       "      <td>191913.72</td>\n",
       "      <td>48365.5</td>\n",
       "      <td>43001.17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1413.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-27 21:00:00</th>\n",
       "      <td>2023</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>75.98</td>\n",
       "      <td>44450.0</td>\n",
       "      <td>89.09</td>\n",
       "      <td>178463.14</td>\n",
       "      <td>47612.0</td>\n",
       "      <td>41629.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2240.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-27 22:00:00</th>\n",
       "      <td>2023</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>79.61</td>\n",
       "      <td>41450.0</td>\n",
       "      <td>88.36</td>\n",
       "      <td>172624.22</td>\n",
       "      <td>44330.5</td>\n",
       "      <td>39404.32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2855.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-27 23:00:00</th>\n",
       "      <td>2023</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>65.54</td>\n",
       "      <td>38550.0</td>\n",
       "      <td>84.05</td>\n",
       "      <td>166243.75</td>\n",
       "      <td>43053.5</td>\n",
       "      <td>38636.27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2997.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Year  Month  Day  FR_Spot  FR_Load  DE_Spot    DE_Load  \\\n",
       "date                                                                          \n",
       "2023-09-27 19:00:00  2023      9   27   106.28  45200.0   107.82  207487.66   \n",
       "2023-09-27 20:00:00  2023      9   27    96.38  43500.0    96.38  191913.72   \n",
       "2023-09-27 21:00:00  2023      9   27    75.98  44450.0    89.09  178463.14   \n",
       "2023-09-27 22:00:00  2023      9   27    79.61  41450.0    88.36  172624.22   \n",
       "2023-09-27 23:00:00  2023      9   27    65.54  38550.0    84.05  166243.75   \n",
       "\n",
       "                     FR_DAH_generation  DE_DAH_generation  \\\n",
       "date                                                        \n",
       "2023-09-27 19:00:00            48853.0           44660.04   \n",
       "2023-09-27 20:00:00            48365.5           43001.17   \n",
       "2023-09-27 21:00:00            47612.0           41629.06   \n",
       "2023-09-27 22:00:00            44330.5           39404.32   \n",
       "2023-09-27 23:00:00            43053.5           38636.27   \n",
       "\n",
       "                     DAH_ImportCapacity_FR_DE  DAH_ImportCapacity_DE_FR  \n",
       "date                                                                     \n",
       "2023-09-27 19:00:00                       0.0                    1812.5  \n",
       "2023-09-27 20:00:00                       0.0                    1413.6  \n",
       "2023-09-27 21:00:00                       0.0                    2240.6  \n",
       "2023-09-27 22:00:00                       0.0                    2855.3  \n",
       "2023-09-27 23:00:00                       0.0                    2997.8  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The input features that are considered in Benchmark model\n",
    "\n",
    "#### EPFtoolbox datasets : \n",
    "https://arxiv.org/pdf/2008.08004.pdf\n",
    "\n",
    "Independently of the model, the available input features to forecast the 24 day-ahead prices of day d,\n",
    "i.e. pd = [pd,1, . . . , pd,24], are the same:\n",
    "\n",
    "    - Historical day-ahead prices of the previous three days and one week ago, i.e. pdâˆ’1, pdâˆ’2, pdâˆ’3, pdâˆ’7.\n",
    "\n",
    "    - The day-ahead forecasts of the two variables of interest (see Section 3 for details) for day d available on day d âˆ’ 1, i.e. x1d = [x1d,1, . . . , x1d,24] and x2d = [x2d,1, . . . , x2d,24] ; note that the variables of interest are different for each market.\n",
    "\n",
    "    - Historical day-ahead forecasts of the variables of interest the previous day and one week ago, i.e. x1dâˆ’1,x1dâˆ’7, x2dâˆ’1, x2dâˆ’7.\n",
    "    \n",
    "    - A dummy variable zd that represents the day of the\n",
    "    week. In the case of the linear model, following\n",
    "    the standard practice in the literature [55, 58, 77],\n",
    "\n",
    "Made in function **_build_and_split_XYs**\n",
    "\n",
    "#### SOTA datasets + enriched data with circular encoding information\n",
    "https://hal.science/hal-03621974v1/document\n",
    "\n",
    "Electricity price datasets are a multivariate time series made of daily\n",
    "data. Those datasets can be reconfigured into a (ð‘‹, ð‘Œ ) couple suitable\n",
    "to learn machine learning models. The predictive data is represented\n",
    "by a two dimensional matrix ð‘‹ âˆˆ Rð‘›ð‘‘Ã—ð‘›ð‘ whose rows represent days\n",
    "and columns are ð‘›ð‘ predictive time-dependent values. The values to\n",
    "be predicted correspond to another matrix ð‘Œ âˆˆ Rð‘›ð‘‘Ã—ð‘›ð‘œ , whose rows\n",
    "also stand for the days and columns are the ð‘›ð‘œ day-ahead prices to be\n",
    "predicted: ð‘Œð‘‘ =(ð‘Œ1ð‘‘+1, â€¦, ð‘Œ ð‘›0ð‘‘+1).\n",
    "\n",
    "To model the time series aspect of the features, ð‘‹ includes the prices of the current day, those of the day\n",
    "before, two days before and the previous week (1, 2, 3 and 7 days lag).\n",
    "Exogenous features are included for the day, the day before and the\n",
    "previous week. In addition to these 240 characteristics, the day of the\n",
    "week is also encoded as an integer and added to the matrix ð‘‹. Indeed,\n",
    "electricity prices are non-stationary time series and exhibit seasonal\n",
    "trends captured by this additional feature. All features (prices and\n",
    "exogenous) are provided with hourly granularity. Thus, the predictive\n",
    "matrix ð‘‹ is as follows:\n",
    "ð‘‹ð‘‘ =(ð‘Œð‘‘âˆ’1, ð‘Œð‘‘âˆ’2, ð‘Œð‘‘âˆ’3, ð‘Œð‘‘âˆ’7, ð¸1ð‘‘, ð¸1ð‘‘âˆ’1, ð¸1ð‘‘âˆ’7,ð¸2ð‘‘, ð¸2ð‘‘âˆ’1, ð¸2ð‘‘âˆ’7,DayOfWeek) with ð‘›ð‘=241.\n",
    "\n",
    "In order to forecast 24-hour prices for the next day, the datasets are\n",
    "reshaped so that for one day ð‘‘, ð‘Œð‘‘\n",
    "contains all 24 prices for the next\n",
    "day: ð‘Œð‘‘ =(ð‘Œ1ð‘‘+1, â€¦, ð‘Œ 24ð‘‘+1).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter optimization\n",
    "For the DNN, as in the original study [59], the input\n",
    "features are optimized together with the hyperparameters using the tree Parzen estimator [108] (see Section\n",
    "4.3 for details).\n",
    "\n",
    "As in the original DNN paper [59], the hyperparameters\n",
    "and input features are optimized together using the treestructured Parzen estimator [108], a Bayesian optimization\n",
    "algorithm based on sequential model-based optimization.\n",
    "To do so, the features are modeled as hyperparameters,\n",
    "with each hyperparameter representing a binary variable\n",
    "that selects whether or not a specific feature is included in\n",
    "the model (as explained in [43]). In more detail, to select\n",
    "which of the 241 available input features are relevant, the\n",
    "method employs 11 decision variables, i.e. 11 hyperparameters:\n",
    "\n",
    "    - Four binary hyperparameters (1-4) that indicate\n",
    "    whether or not to include the historical day ahead\n",
    "    prices pdâˆ’1, pdâˆ’2, pdâˆ’3, pdâˆ’7. The selection is done\n",
    "    per day12, e.g. the algorithm either selects all the\n",
    "    prices pdâˆ’j of j days ago or it cannot select any price\n",
    "    from day d âˆ’ j, hence the four hyperparameters.\n",
    "\n",
    "    - Two binary hyperparameters (5-6) that indicate\n",
    "    whether or not to include each of the day-ahead forecasts x1d and x2d. As with the past prices, this is done for the whole day, i.e. a hyperparameter either selects all the elements in xjd or none.\n",
    "\n",
    "    - Four binary hyperparameters (7-10) that indicate\n",
    "    whether or not to include the historical day-ahead\n",
    "    forecasts x1dâˆ’1, x2dâˆ’1, x1dâˆ’7, and x2dâˆ’7. This selection is also done per day.\n",
    "\n",
    "    - One binary hyperparameter (11) that indicates\n",
    "    whether or not to include the variable zd representing\n",
    "    the day of the week.\n",
    "In short, 10 binary hyperparameters indicating whether or\n",
    "not to include 24 inputs each and another binary hyperparameter indicating whether or not to include a dummy\n",
    "variable.\n",
    "\n",
    "Besides selecting the features, the algorithm also optimizes eight additional hyperparameters:\n",
    "1) the number of neurons per layer\n",
    "2) the activation function\n",
    "3) the dropout rate\n",
    "4) the learning rate\n",
    "5) whether or not to\n",
    "use batch normalization\n",
    "6) the type of data preprocessing technique\n",
    "7) the initialization of the DNN weights\n",
    "8) the coefficient for L1 regularization that is applied to each\n",
    "layerâ€™s kernel.\n",
    "\n",
    "Unlike the weights of the DNN that are recalibrated on a\n",
    "daily basis, the hyperparameter and features are optimized\n",
    "only once using the four years of data prior to the testing\n",
    "period. \n",
    "\n",
    "It is important to note that the algorithm runs\n",
    "for a number T of iterations, where at every iteration the\n",
    "algorithm infers a potential optimal subset of hyperparameters/features and evaluates this subset in the validation dataset.\n",
    "\n",
    "For the proposed open-access benchmark models, T is selected as 1500 iterations to obtain a trade-off\n",
    "between accuracy and computational requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-01-01 00:00:00\n",
      "2023-12-31 23:00:00\n"
     ]
    }
   ],
   "source": [
    "X = pd.read_csv(\"datasets/FR_readytodnn.csv\", index_col=0, parse_dates=True)\n",
    "min_date = X.index.min()\n",
    "print(min_date)\n",
    "max_date = X.index.max()\n",
    "print(max_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matis\\anaconda3\\envs\\ds\\lib\\site-packages\\epftoolbox\\data\\_datasets.py:146: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  begin_test_date = pd.to_datetime(begin_test_date, dayfirst=True)\n",
      "c:\\Users\\matis\\anaconda3\\envs\\ds\\lib\\site-packages\\epftoolbox\\data\\_datasets.py:147: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  end_test_date = pd.to_datetime(end_test_date, dayfirst=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test datasets: 2022-12-31 00:00:00 - 2023-12-31 23:00:00\n",
      "WARNING:tensorflow:From c:\\Users\\matis\\anaconda3\\envs\\ds\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\matis\\anaconda3\\envs\\ds\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\matis\\anaconda3\\envs\\ds\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Tested 1/1500 iterations.\n",
      "Best MAE - Validation Dataset\n",
      "  MAE: 38.6 | sMAPE: 33.67 %\n",
      "\n",
      "Best MAE - Test Dataset\n",
      "  MAE: 24.4 | sMAPE: 34.54 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Tested 2/1500 iterations.\n",
      "Best MAE - Validation Dataset\n",
      "  MAE: 38.6 | sMAPE: 33.67 %\n",
      "\n",
      "Best MAE - Test Dataset\n",
      "  MAE: 24.4 | sMAPE: 34.54 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Tested 3/1500 iterations.\n",
      "Best MAE - Validation Dataset\n",
      "  MAE: 38.6 | sMAPE: 33.67 %\n",
      "\n",
      "Best MAE - Test Dataset\n",
      "  MAE: 24.4 | sMAPE: 34.54 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matis\\anaconda3\\envs\\ds\\lib\\site-packages\\epftoolbox\\data\\_wrangling.py:49: RuntimeWarning: divide by zero encountered in divide\n",
      "  transformed_data[:, i] = (data[:, i] - self.median[i]) / self.mad[i]\n",
      "c:\\Users\\matis\\anaconda3\\envs\\ds\\lib\\site-packages\\epftoolbox\\data\\_wrangling.py:49: RuntimeWarning: invalid value encountered in divide\n",
      "  transformed_data[:, i] = (data[:, i] - self.median[i]) / self.mad[i]\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Tested 4/1500 iterations.\n",
      "Best MAE - Validation Dataset\n",
      "  MAE: 38.6 | sMAPE: 33.67 %\n",
      "\n",
      "Best MAE - Test Dataset\n",
      "  MAE: 24.4 | sMAPE: 34.54 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matis\\anaconda3\\envs\\ds\\lib\\site-packages\\epftoolbox\\data\\_wrangling.py:49: RuntimeWarning: divide by zero encountered in divide\n",
      "  transformed_data[:, i] = (data[:, i] - self.median[i]) / self.mad[i]\n",
      "c:\\Users\\matis\\anaconda3\\envs\\ds\\lib\\site-packages\\epftoolbox\\data\\_wrangling.py:49: RuntimeWarning: invalid value encountered in divide\n",
      "  transformed_data[:, i] = (data[:, i] - self.median[i]) / self.mad[i]\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[149], line 48\u001b[0m\n\u001b[0;32m     39\u001b[0m path_hyperparameters_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./experimental_files/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Check documentation of the hyperparameter_optimizer for each of the function parameters\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# In this example, we optimize a model for the PJM market.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# We consider two directories, one for storing the datasets and the other one for the experimental files.\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# We start a hyperparameter optimization from scratch. We employ 1500 iterations in hyperopt,\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# 1 year of test data, a DNN with 2 hidden layers, a calibration window of 4 years,\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# we avoid data augmentation,  and we provide an experiment_id equal to 1\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m \u001b[43mhyperparameter_optimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_datasets_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_datasets_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mpath_hyperparameters_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_hyperparameters_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mnew_hyperopt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_hyperopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnlayers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnlayers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m                         \u001b[49m\u001b[43myears_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43myears_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcalibration_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcalibration_window\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mshuffle_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_augmentation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mbegin_test_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbegin_test_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_test_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend_test_date\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\matis\\anaconda3\\envs\\ds\\lib\\site-packages\\epftoolbox\\models\\_dnn_hyperopt.py:320\u001b[0m, in \u001b[0;36mhyperparameter_optimizer\u001b[1;34m(path_datasets_folder, path_hyperparameters_folder, new_hyperopt, max_evals, nlayers, dataset, years_test, calibration_window, shuffle_train, data_augmentation, experiment_id, begin_test_date, end_test_date)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;66;03m# Perform hyperparameter optimization\u001b[39;00m\n\u001b[0;32m    314\u001b[0m fmin_objective \u001b[38;5;241m=\u001b[39m partial(_hyperopt_objective, trials\u001b[38;5;241m=\u001b[39mtrials, trials_file_path\u001b[38;5;241m=\u001b[39mtrials_file_path, \n\u001b[0;32m    315\u001b[0m                          max_evals\u001b[38;5;241m=\u001b[39mmax_evals, nlayers\u001b[38;5;241m=\u001b[39mnlayers, dfTrain\u001b[38;5;241m=\u001b[39mdfTrain, dfTest\u001b[38;5;241m=\u001b[39mdfTest, \n\u001b[0;32m    316\u001b[0m                          shuffle_train\u001b[38;5;241m=\u001b[39mshuffle_train, dataset\u001b[38;5;241m=\u001b[39mdataset, \n\u001b[0;32m    317\u001b[0m                          data_augmentation\u001b[38;5;241m=\u001b[39mdata_augmentation, calibration_window\u001b[38;5;241m=\u001b[39mcalibration_window,\n\u001b[0;32m    318\u001b[0m                          n_exogenous_inputs\u001b[38;5;241m=\u001b[39mn_exogenous_inputs)\n\u001b[1;32m--> 320\u001b[0m \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfmin_objective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\matis\\anaconda3\\envs\\ds\\lib\\site-packages\\hyperopt\\fmin.py:540\u001b[0m, in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    537\u001b[0m     fn \u001b[38;5;241m=\u001b[39m __objective_fmin_wrapper(fn)\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_trials_fmin \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(trials, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfmin\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrials\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m        \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_queue_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_argmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stop_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrials_save_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(trials_save_file):\n",
      "File \u001b[1;32mc:\\Users\\matis\\anaconda3\\envs\\ds\\lib\\site-packages\\hyperopt\\base.py:671\u001b[0m, in \u001b[0;36mTrials.fmin\u001b[1;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;66;03m# -- Stop-gap implementation!\u001b[39;00m\n\u001b[0;32m    667\u001b[0m \u001b[38;5;66;03m#    fmin should have been a Trials method in the first place\u001b[39;00m\n\u001b[0;32m    668\u001b[0m \u001b[38;5;66;03m#    but for now it's still sitting in another file.\u001b[39;00m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfmin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fmin\n\u001b[1;32m--> 671\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_trials_fmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# -- prevent recursion\u001b[39;49;00m\n\u001b[0;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_argmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stop_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrials_save_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\matis\\anaconda3\\envs\\ds\\lib\\site-packages\\hyperopt\\fmin.py:586\u001b[0m, in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    583\u001b[0m rval\u001b[38;5;241m.\u001b[39mcatch_eval_exceptions \u001b[38;5;241m=\u001b[39m catch_eval_exceptions\n\u001b[0;32m    585\u001b[0m \u001b[38;5;66;03m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[1;32m--> 586\u001b[0m \u001b[43mrval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexhaust\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_argmin:\n\u001b[0;32m    589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trials\u001b[38;5;241m.\u001b[39mtrials) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\matis\\anaconda3\\envs\\ds\\lib\\site-packages\\hyperopt\\fmin.py:364\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexhaust\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    363\u001b[0m     n_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials)\n\u001b[1;32m--> 364\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_done\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_until_done\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masynchronous\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\matis\\anaconda3\\envs\\ds\\lib\\site-packages\\hyperopt\\fmin.py:300\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    297\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoll_interval_secs)\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;66;03m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserial_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials_save_file \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\matis\\anaconda3\\envs\\ds\\lib\\site-packages\\hyperopt\\fmin.py:178\u001b[0m, in \u001b[0;36mFMinIter.serial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    176\u001b[0m ctrl \u001b[38;5;241m=\u001b[39m base\u001b[38;5;241m.\u001b[39mCtrl(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials, current_trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdomain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctrl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    180\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob exception: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[1;32mc:\\Users\\matis\\anaconda3\\envs\\ds\\lib\\site-packages\\hyperopt\\base.py:892\u001b[0m, in \u001b[0;36mDomain.evaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;66;03m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[0;32m    885\u001b[0m     \u001b[38;5;66;03m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[0;32m    886\u001b[0m     \u001b[38;5;66;03m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[0;32m    887\u001b[0m     pyll_rval \u001b[38;5;241m=\u001b[39m pyll\u001b[38;5;241m.\u001b[39mrec_eval(\n\u001b[0;32m    888\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpr,\n\u001b[0;32m    889\u001b[0m         memo\u001b[38;5;241m=\u001b[39mmemo,\n\u001b[0;32m    890\u001b[0m         print_node_on_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrec_eval_print_node_on_error,\n\u001b[0;32m    891\u001b[0m     )\n\u001b[1;32m--> 892\u001b[0m     rval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyll_rval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rval, (\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39mnumber)):\n\u001b[0;32m    895\u001b[0m     dict_rval \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(rval), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m: STATUS_OK}\n",
      "File \u001b[1;32mc:\\Users\\matis\\anaconda3\\envs\\ds\\lib\\site-packages\\epftoolbox\\models\\_dnn_hyperopt.py:183\u001b[0m, in \u001b[0;36m_hyperopt_objective\u001b[1;34m(hyperparameters, trials, trials_file_path, max_evals, nlayers, dfTrain, dfTest, shuffle_train, dataset, data_augmentation, calibration_window, n_exogenous_inputs)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# Initialize model\u001b[39;00m\n\u001b[0;32m    174\u001b[0m forecaster \u001b[38;5;241m=\u001b[39m DNNModel(neurons\u001b[38;5;241m=\u001b[39mneurons, n_features\u001b[38;5;241m=\u001b[39mXtrain\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \n\u001b[0;32m    175\u001b[0m                  dropout\u001b[38;5;241m=\u001b[39mhyperparameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m'\u001b[39m], batch_normalization\u001b[38;5;241m=\u001b[39mhyperparameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_normalization\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[0;32m    176\u001b[0m                  lr\u001b[38;5;241m=\u001b[39mhyperparameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m], verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    180\u001b[0m                  lambda_reg\u001b[38;5;241m=\u001b[39mhyperparameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreg\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlambda\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    181\u001b[0m                  initializer\u001b[38;5;241m=\u001b[39mhyperparameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 183\u001b[0m \u001b[43mforecaster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mYtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mYval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m Yp \u001b[38;5;241m=\u001b[39m forecaster\u001b[38;5;241m.\u001b[39mpredict(Xval)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hyperparameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaleY\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNorm\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNorm1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStd\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMedian\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvariant\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\matis\\anaconda3\\envs\\ds\\lib\\site-packages\\epftoolbox\\models\\_dnn.py:270\u001b[0m, in \u001b[0;36mDNNModel.fit\u001b[1;34m(self, trainX, trainY, valX, valY)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[0;32m    268\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 270\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m192\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;66;03m# Updating epoch metrics and displaying useful information\u001b[39;00m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n",
      "File \u001b[1;32mc:\\Users\\matis\\anaconda3\\envs\\ds\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\matis\\anaconda3\\envs\\ds\\lib\\site-packages\\keras\\src\\engine\\training.py:1745\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1735\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cluster_coordinator \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1736\u001b[0m         tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mcoordinator\u001b[38;5;241m.\u001b[39mClusterCoordinator(\n\u001b[0;32m   1737\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\n\u001b[0;32m   1738\u001b[0m         )\n\u001b[0;32m   1739\u001b[0m     )\n\u001b[0;32m   1741\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mscope(), training_utils\u001b[38;5;241m.\u001b[39mRespectCompiledTrainableState(  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m   1742\u001b[0m     \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m   1743\u001b[0m ):\n\u001b[0;32m   1744\u001b[0m     \u001b[38;5;66;03m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[39;00m\n\u001b[1;32m-> 1745\u001b[0m     data_handler \u001b[38;5;241m=\u001b[39m \u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1746\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1747\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1748\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1749\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1750\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1751\u001b[0m \u001b[43m        \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1752\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1753\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1754\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1755\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1756\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1757\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1758\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1759\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1760\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1762\u001b[0m     \u001b[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32mc:\\Users\\matis\\anaconda3\\envs\\ds\\lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1688\u001b[0m, in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1686\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorExactEvalDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1687\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1688\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\matis\\anaconda3\\envs\\ds\\lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1292\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute, pss_evaluation_shards)\u001b[0m\n\u001b[0;32m   1289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution \u001b[38;5;241m=\u001b[39m steps_per_execution\n\u001b[0;32m   1291\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m select_data_adapter(x, y)\n\u001b[1;32m-> 1292\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter \u001b[38;5;241m=\u001b[39m \u001b[43madapter_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribution_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpss_evaluation_shards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpss_evaluation_shards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1306\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1308\u001b[0m strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy()\n\u001b[0;32m   1310\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\matis\\anaconda3\\envs\\ds\\lib\\site-packages\\keras\\src\\engine\\data_adapter.py:253\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    242\u001b[0m     x,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    251\u001b[0m ):\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(x, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 253\u001b[0m     x, y, sample_weights \u001b[38;5;241m=\u001b[39m \u001b[43m_process_tensorlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m     sample_weight_modes \u001b[38;5;241m=\u001b[39m broadcast_sample_weight_modes(\n\u001b[0;32m    255\u001b[0m         sample_weights, sample_weight_modes\n\u001b[0;32m    256\u001b[0m     )\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;66;03m# If sample_weights are not specified for an output use 1.0 as weights.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\matis\\anaconda3\\envs\\ds\\lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1163\u001b[0m, in \u001b[0;36m_process_tensorlike\u001b[1;34m(inputs)\u001b[0m\n\u001b[0;32m   1160\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _scipy_sparse_to_sparse_tensor(x)\n\u001b[0;32m   1161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m-> 1163\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_convert_single_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mlist_to_tuple(inputs)\n",
      "File \u001b[1;32mc:\\Users\\matis\\anaconda3\\envs\\ds\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:631\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnest.map_structure\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap_structure\u001b[39m(func, \u001b[38;5;241m*\u001b[39mstructure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    547\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new structure by applying `func` to each atom in `structure`.\u001b[39;00m\n\u001b[0;32m    548\u001b[0m \n\u001b[0;32m    549\u001b[0m \u001b[38;5;124;03m  Refer to [tf.nest](https://www.tensorflow.org/api_docs/python/tf/nest)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;124;03m    ValueError: If wrong keyword arguments are provided.\u001b[39;00m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m nest_util\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[0;32m    632\u001b[0m       nest_util\u001b[38;5;241m.\u001b[39mModality\u001b[38;5;241m.\u001b[39mCORE, func, \u001b[38;5;241m*\u001b[39mstructure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    633\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\matis\\anaconda3\\envs\\ds\\lib\\site-packages\\tensorflow\\python\\util\\nest_util.py:1066\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(modality, func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    969\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new structure by applying `func` to each atom in `structure`.\u001b[39;00m\n\u001b[0;32m    970\u001b[0m \n\u001b[0;32m    971\u001b[0m \u001b[38;5;124;03m- For Modality.CORE: Refer to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;124;03m  ValueError: If wrong keyword arguments are provided.\u001b[39;00m\n\u001b[0;32m   1064\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1065\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m modality \u001b[38;5;241m==\u001b[39m Modality\u001b[38;5;241m.\u001b[39mCORE:\n\u001b[1;32m-> 1066\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _tf_core_map_structure(func, \u001b[38;5;241m*\u001b[39mstructure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1067\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m modality \u001b[38;5;241m==\u001b[39m Modality\u001b[38;5;241m.\u001b[39mDATA:\n\u001b[0;32m   1068\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _tf_data_map_structure(func, \u001b[38;5;241m*\u001b[39mstructure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\matis\\anaconda3\\envs\\ds\\lib\\site-packages\\tensorflow\\python\\util\\nest_util.py:1106\u001b[0m, in \u001b[0;36m_tf_core_map_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m   1101\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (_tf_core_flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m   1102\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _tf_core_pack_sequence_as(\n\u001b[0;32m   1105\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m-> 1106\u001b[0m     [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m   1107\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites,\n\u001b[0;32m   1108\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\matis\\anaconda3\\envs\\ds\\lib\\site-packages\\tensorflow\\python\\util\\nest_util.py:1106\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1101\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (_tf_core_flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m   1102\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _tf_core_pack_sequence_as(\n\u001b[0;32m   1105\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m-> 1106\u001b[0m     [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m   1107\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites,\n\u001b[0;32m   1108\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\matis\\anaconda3\\envs\\ds\\lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1158\u001b[0m, in \u001b[0;36m_process_tensorlike.<locals>._convert_single_tensor\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1156\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(x\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, np\u001b[38;5;241m.\u001b[39mfloating):\n\u001b[0;32m   1157\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mfloatx()\n\u001b[1;32m-> 1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1159\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _is_scipy_sparse(x):\n\u001b[0;32m   1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _scipy_sparse_to_sparse_tensor(x)\n",
      "File \u001b[1;32mc:\\Users\\matis\\anaconda3\\envs\\ds\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\matis\\anaconda3\\envs\\ds\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1258\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1260\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1262\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1263\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1264\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\matis\\anaconda3\\envs\\ds\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion.py:161\u001b[0m, in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[1;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m\u001b[38;5;241m.\u001b[39mtf_export(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvert_to_tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[0;32m     97\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_to_tensor_v2_with_dispatch\u001b[39m(\n\u001b[0;32m     99\u001b[0m     value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype_hint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    100\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m tensor_lib\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    101\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Converts the given `value` to a `Tensor`.\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m  This function converts Python objects of various types to `Tensor`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;124;03m    ValueError: If the `value` is a tensor not of given `dtype` in graph mode.\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 161\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_to_tensor_v2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype_hint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_hint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\matis\\anaconda3\\envs\\ds\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion.py:171\u001b[0m, in \u001b[0;36mconvert_to_tensor_v2\u001b[1;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Converts the given `value` to a `Tensor`.\"\"\"\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# preferred_dtype = preferred_dtype or dtype_hint\u001b[39;00m\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor_conversion_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_hint\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\matis\\anaconda3\\envs\\ds\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py:234\u001b[0m, in \u001b[0;36mconvert\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[0;32m    225\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    226\u001b[0m           _add_error_prefix(\n\u001b[0;32m    227\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    230\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    231\u001b[0m               name\u001b[38;5;241m=\u001b[39mname))\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 234\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[0;32m    237\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\matis\\anaconda3\\envs\\ds\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:335\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_tensor_conversion_function\u001b[39m(v, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    333\u001b[0m                                          as_ref\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    334\u001b[0m   _ \u001b[38;5;241m=\u001b[39m as_ref\n\u001b[1;32m--> 335\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\matis\\anaconda3\\envs\\ds\\lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py:142\u001b[0m, in \u001b[0;36mweak_tensor_binary_op_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    141\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    143\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    144\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[1;32mc:\\Users\\matis\\anaconda3\\envs\\ds\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:271\u001b[0m, in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(\n\u001b[0;32m    174\u001b[0m     value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    175\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ops\u001b[38;5;241m.\u001b[39mOperation, ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase]:\n\u001b[0;32m    176\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 271\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\matis\\anaconda3\\envs\\ds\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:284\u001b[0m, in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    283\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m--> 284\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_eager_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    286\u001b[0m const_tensor \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39m_create_graph_constant(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     value, dtype, shape, name, verify_shape, allow_broadcast\n\u001b[0;32m    288\u001b[0m )\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m const_tensor\n",
      "File \u001b[1;32mc:\\Users\\matis\\anaconda3\\envs\\ds\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:296\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_eager_impl\u001b[39m(\n\u001b[0;32m    293\u001b[0m     ctx, value, dtype, shape, verify_shape\n\u001b[0;32m    294\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase:\n\u001b[0;32m    295\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 296\u001b[0m   t \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_eager_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    297\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[1;32mc:\\Users\\matis\\anaconda3\\envs\\ds\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    101\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    102\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from epftoolbox.models import hyperparameter_optimizer\n",
    "\n",
    "# Number of layers in DNN\n",
    "nlayers = 2\n",
    "\n",
    "# Market under study. If it not one of the standard ones, the file name\n",
    "# has to be provided, where the file has to be a csv file\n",
    "dataset = 'FR_readytodnn'\n",
    "\n",
    "# Number of years (a year is 364 days) in the test dataset.\n",
    "years_test = 1\n",
    "\n",
    "# Optional parameters for selecting the test dataset, if either of them is not provided, \n",
    "# the test dataset is built using the years_test parameter. They should either be one of\n",
    "# the date formats existing in python or a string with the following format\n",
    "# \"%d/%m/%Y %H:%M\"\n",
    "begin_test_date = \"2022-12-31\"\n",
    "end_test_date = \"2023-12-31\"\n",
    "\n",
    "# Boolean that selects whether the validation and training datasets are shuffled\n",
    "shuffle_train = 1\n",
    "\n",
    "# Boolean that selects whether a data augmentation technique for DNNs is used\n",
    "data_augmentation = 0\n",
    "\n",
    "# Boolean that selects whether we start a new hyperparameter optimization or we restart an existing one\n",
    "new_hyperopt = 1\n",
    "\n",
    "# Number of years used in the training dataset for recalibration\n",
    "calibration_window = 4\n",
    "\n",
    "# Unique identifier to read the trials file of hyperparameter optimization\n",
    "experiment_id = 1\n",
    "\n",
    "# Number of iterations for hyperparameter optimization\n",
    "# It can be empirically observed that the performance of the models barely improves after 1000 iterations. \n",
    "# Moreover, performing 1500 iteration takes approximately just one day on a regular quadcore\n",
    "# laptop like the i7-6920HQ, a computation cost very acceptable when\n",
    "# the algorithm has to run only once\n",
    "max_evals = 1500\n",
    "\n",
    "path_datasets_folder = \"./datasets/\"\n",
    "path_hyperparameters_folder = \"./experimental_files/\"\n",
    "\n",
    "\n",
    "# Check documentation of the hyperparameter_optimizer for each of the function parameters\n",
    "# In this example, we optimize a model for the PJM market.\n",
    "# We consider two directories, one for storing the datasets and the other one for the experimental files.\n",
    "# We start a hyperparameter optimization from scratch. We employ 1500 iterations in hyperopt,\n",
    "# 1 year of test data, a DNN with 2 hidden layers, a calibration window of 4 years,\n",
    "# we avoid data augmentation,  and we provide an experiment_id equal to 1\n",
    "hyperparameter_optimizer(path_datasets_folder=path_datasets_folder, \n",
    "                         path_hyperparameters_folder=path_hyperparameters_folder, \n",
    "                         new_hyperopt=new_hyperopt, max_evals=max_evals, nlayers=nlayers, dataset=dataset, \n",
    "                         years_test=years_test, calibration_window=calibration_window, \n",
    "                         shuffle_train=shuffle_train, data_augmentation=0, experiment_id=experiment_id,\n",
    "                         begin_test_date=begin_test_date, end_test_date=end_test_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checklist to ensure adequate EPF research :\n",
    "\n",
    "As a final contribution, and with the goal of facilitating\n",
    "the work of reviewers of future EPF publications, we provide a short checklist to evaluate whether any new research in EPF satisfies the requirements to be reproducible and to lead to meaningful conclusions:\n",
    "\n",
    "    â€¢ The test dataset comprises at least a year of data.\n",
    "    â€¢ Any new model is tested against state-of-the-art open-access models, e.g. the ones provided here.\n",
    "    â€¢ The computational cost of new methods is evaluated\n",
    "    and compared against the computational cost of existing methods.\n",
    "    â€¢ The employed datasets are open-access.\n",
    "    â€¢ The study is based on multiple markets.\n",
    "    â€¢ rMAE is employed as one of the accuracy metrics to\n",
    "    evaluate forecasting accuracy.\n",
    "    â€¢ Statistical testing is used to assess whether differences\n",
    "    in performance are significant.\n",
    "    â€¢ Forecasting models are recalibrated on a daily basis\n",
    "    and not simply estimated once and evaluated in the\n",
    "    full out-of-sample dataset.\n",
    "    â€¢ Hyperparameters are estimated using a validation\n",
    "    dataset that is different from the test dataset.\n",
    "    â€¢ The split and dates of the dataset are explicitly stated.\n",
    "    â€¢ All the inputs of the model are explicitly defined\n",
    "    - The test dataset is selected as the last section of the\n",
    "    full dataset and does not contain any overlapping data\n",
    "    with the training or validation datasets.\n",
    "    â€¢ State-of-the-art and free toolboxes are used for modeling the benchmark models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
